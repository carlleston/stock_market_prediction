{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200709005250-0001\n",
      "KERNEL_ID = 0bfaedb9-d14e-4cb1-ae84-b27dabb970ac\n"
     ]
    }
   ],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(spark.sparkContext, '*******-****-****-****-************', 'p-************************************')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment: Spark 2.4 & Python 3.6\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "\n",
    "################# Spark ML #############################\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import percent_rank\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-*********-****-****-******-**********',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': '******************************************'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_*************************_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data_3 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('ts_data.csv', '************-donotdelete-**-***************'))\n",
    "\n",
    "df_ts = df_data_3\n",
    "df_ts = df_ts[['Date','Itau_open','BVSP_open','USDBRL_open','Itau_Close','lag_1','lag_2','lag_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_4 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('feature_news.csv', '************-donotdelete-**-***************'))\n",
    "\n",
    "\n",
    "df_news = df_data_4\n",
    "df_news = df_news[['Date', 'Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ts.alias('a').join(df_news.alias('b'), on = ['Date'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "df = df.withColumn('Date',to_date(unix_timestamp(col('Date'), 'yyyy-MM-dd').cast(\"timestamp\"))).orderBy('Date')\n",
    "df = df.withColumn('Itau_open', col('Itau_open').cast('double'))\n",
    "df = df.withColumn('BVSP_open', col('BVSP_open').cast('double'))\n",
    "df = df.withColumn('USDBRL_open', col('USDBRL_open').cast('double'))\n",
    "df = df.withColumn('Itau_Close', col('Itau_Close').cast('double'))\n",
    "df = df.withColumn('lag_1', col('lag_1').cast('double'))\n",
    "df = df.withColumn('lag_2', col('lag_2').cast('double'))\n",
    "df = df.withColumn('lag_3', col('lag_3').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as func\n",
    "import sys\n",
    "\n",
    "window_ff = Window.orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "read_last = func.last(df['Class'],  \n",
    "                      ignorenulls=True)\\\n",
    "                .over(window_ff)\n",
    "# add columns to the dataframe\n",
    "df = df.withColumn('Class', read_last)\n",
    "df = df.na.fill({'Class': 'NN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Dummies News Classes\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "categories = df.select('Class').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "categories.sort()\n",
    "for category in categories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'class'+'_'+category\n",
    "    df = df.withColumn(new_column_name, function(col('class')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[['Date','Itau_open','BVSP_open','USDBRL_open','Itau_Close','class_N','class_NN','class_P','lag_1','lag_2','lag_3']]\n",
    "\n",
    "FEATURES_COL1 = ['Itau_open','BVSP_open','USDBRL_open','lag_1','lag_2','lag_3']\n",
    "vectorAssembler = VectorAssembler(inputCols=FEATURES_COL1,outputCol=\"features\")\n",
    "vdf = vectorAssembler.transform(df.na.drop())\n",
    "vdf = vdf.select(['Date','Itau_Close','features','class_N','class_NN','class_P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_features = MinMaxScaler(inputCol= 'features', outputCol= 'scaled_features')\n",
    "model_scale = scale_features.fit(vdf)\n",
    "df_scaled = model_scale.transform(vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COL1 = ['scaled_features','class_N','class_NN','class_P']\n",
    "vectorAssembler = VectorAssembler(inputCols=FEATURES_COL1,outputCol=\"Col_features\")\n",
    "df_completed = vectorAssembler.transform(df_scaled)\n",
    "df_completed = df_completed.select(['Date','Itau_Close','Col_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed = df_completed.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"Date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_completed.where(\"rank <= .95\").drop(\"rank\")\n",
    "test_df = df_completed.where(\"rank > .95\").drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'Col_features', labelCol='Itau_Close')\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.989227\n"
     ]
    }
   ],
   "source": [
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"Itau_Close\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.605064\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|        prediction|        Itau_Close|        Col_features|\n",
      "+------------------+------------------+--------------------+\n",
      "| 34.59906994251565|  34.2400016784668|[0.87112550780915...|\n",
      "| 34.24067705490668| 34.79999923706055|[0.85966997581227...|\n",
      "|34.750114217700606|35.880001068115234|[0.87589868660350...|\n",
      "|35.564205125912665| 36.38999938964844|[0.90199178886883...|\n",
      "|  36.4278997689138|36.560001373291016|[0.92967591026995...|\n",
      "|36.555032855445546| 36.79999923706055|[0.93253979326917...|\n",
      "| 36.65360207856043| 37.29999923706055|[0.93572195292773...|\n",
      "| 37.09134911556927| 36.63999938964844|[0.94972309126448...|\n",
      "| 36.47460822948598|36.869998931884766|[0.92999418692930...|\n",
      "| 36.30601535738676| 36.22999954223633|[0.92458457620323...|\n",
      "| 36.66381350208888|36.349998474121094|[0.93604010820010...|\n",
      "| 36.45502852253872|36.470001220703125|[0.92935775499758...|\n",
      "|  37.6014961375709|37.099998474121094|[0.96722454453215...|\n",
      "|  37.0639706513024|36.599998474121094|[0.95004124653684...|\n",
      "| 36.74833779179345| 36.81999969482422|[0.93985857840340...|\n",
      "| 36.33228984329988| 36.27000045776367|[0.92649387199837...|\n",
      "|  36.1726079591075| 36.43000030517578|[0.92140253793164...|\n",
      "| 35.99487353585641|35.810001373291016|[0.91567477193321...|\n",
      "| 35.64595926696138| 35.45000076293945|[0.90453739520870...|\n",
      "| 35.47633948706174| 35.68000030517578|[0.89912790586961...|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"Itau_Close\",\"Col_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'predictions.csv',\n",
       " 'message': 'File saved to project storage.',\n",
       " 'bucket_name': 'ibmcapstoneitub4tsforecastingusin-donotdelete-pr-mnxk95wtdc2z0q',\n",
       " 'asset_id': '98e36754-787e-423a-b719-024c459b64f6'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = predictions.toPandas()\n",
    "project.save_data(data=df_pred.to_csv(), file_name='predictions.csv',overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
